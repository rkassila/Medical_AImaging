{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6dd359-89bb-4af2-b7c5-1854db762696",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import keras\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from transformers import TFAutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50, imagenet_utils\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, concatenate, UpSampling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d2a3b18-3f64-4986-9ea3-745e73aa9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import ViTFeatureExtractor, TFAutoModelForImageClassification\n",
    "\n",
    "# Define your URLs and limits\n",
    "url_normal = '../data/MR/knee/normal_filter/'\n",
    "url_airspace = '../data/MR/knee/acl_pathology_filter/'\n",
    "url_bronch = '../data/MR/knee/bone_inflammation_filter/'\n",
    "url_inter = '../data/MR/knee/chondral_abnormality/'\n",
    "url_nodule = '../data/MR/knee/fracture_filter/'\n",
    "url_parenchyma = '../data/MR/knee/hematoma_filter/'\n",
    "\n",
    "limite = 100  # Adjust the limit as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f06562b5-f6b6-45ff-83c6-542b4ba13983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "images_normal = [cv2.imread(file) for file in glob.glob(url_normal + \"*.png\")][:limite]\n",
    "images_airspace = [cv2.imread(file) for file in glob.glob(url_airspace + \"*.png\")][:limite]\n",
    "images_bronch = [cv2.imread(file) for file in glob.glob(url_bronch + \"*.png\")][:limite]\n",
    "images_inter = [cv2.imread(file) for file in glob.glob(url_inter + \"*.png\")][:limite]\n",
    "images_nodule = [cv2.imread(file) for file in glob.glob(url_nodule + \"*.png\")][:limite]\n",
    "images_parenchyma = [cv2.imread(file) for file in glob.glob(url_parenchyma + \"*.png\")][:limite]\n",
    "\n",
    "# Assign labels\n",
    "labels_normal = [0] * len(images_normal)\n",
    "labels_airspace = [1] * len(images_airspace)\n",
    "labels_bronch = [2] * len(images_bronch)\n",
    "labels_inter = [3] * len(images_inter)\n",
    "labels_nodule = [4] * len(images_nodule)\n",
    "labels_parenchyma = [5] * len(images_parenchyma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c02ed-c27e-42b8-aa4e-d20647106bc6",
   "metadata": {},
   "source": [
    "# Other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1656f1e7-d697-43a9-8021-f91917988873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/bias:0', 'vit/pooler/dense/kernel:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_vi_t_for_image_classifi  TFSequenceClassifierOut   85803270  \n",
      " cation_22 (TFViTForImageCl  put(loss=None, logits=(             \n",
      " assification)               None, 6),                           \n",
      "                              hidden_states=None, at             \n",
      "                             tentions=None)                      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               1792      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85806604 (327.33 MB)\n",
      "Trainable params: 3334 (13.02 KB)\n",
      "Non-trainable params: 85803270 (327.31 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, TFViTForImageClassification\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_train, X_val, and X_test are your original datasets\n",
    "\n",
    "# Concatenate your data\n",
    "X = np.concatenate((images_normal, images_airspace, images_bronch, images_inter, images_nodule, images_parenchyma), axis=0)\n",
    "y = np.concatenate((labels_normal, labels_airspace, labels_bronch, labels_inter, labels_nodule, labels_parenchyma), axis=0)\n",
    "\n",
    "# Split your data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5)\n",
    "\n",
    "# Use the ViT feature extractor\n",
    "vit_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')  # Replace with the actual pretrained model name\n",
    "\n",
    "X_train_vit = vit_feature_extractor(X_train, return_tensors=\"np\", padding=True, truncation=True)\n",
    "X_val_vit = vit_feature_extractor(X_val, return_tensors=\"np\", padding=True, truncation=True)\n",
    "X_test_vit = vit_feature_extractor(X_test, return_tensors=\"np\", padding=True, truncation=True)\n",
    "\n",
    "# Reshape the input to match the expected shape of the ViT model\n",
    "X_train_vit_reshaped = X_train_vit['pixel_values']\n",
    "X_val_vit_reshaped = X_val_vit['pixel_values']\n",
    "X_test_vit_reshaped = X_test_vit['pixel_values']\n",
    "\n",
    "# Build a simple model\n",
    "num_classes = 6  # Adjust based on the number of classes in your data\n",
    "\n",
    "# Assuming X_train_vit_reshaped is the input shape for your model\n",
    "input_shape = X_train_vit_reshaped.shape[1:]\n",
    "\n",
    "# Create the ViT model\n",
    "vit_model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=6)\n",
    "\n",
    "# Set the layers of the ViT model to be non-trainable\n",
    "for layer in vit_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a Sequential model with the ViT model and a Dense layer with softmax activation\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),  # Input layer with the correct shape\n",
    "    vit_model,  # ViT model\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # Dense layer with softmax activation for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f349247a-9003-451f-a3a2-d6739a1d4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 132s 9s/step - loss: 1.7882 - accuracy: 0.2081 - val_loss: 1.7872 - val_accuracy: 0.1294\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 112s 9s/step - loss: 1.7781 - accuracy: 0.2259 - val_loss: 1.7865 - val_accuracy: 0.1176\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 120s 9s/step - loss: 1.7706 - accuracy: 0.2284 - val_loss: 1.7841 - val_accuracy: 0.1294\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 108s 8s/step - loss: 1.7642 - accuracy: 0.2335 - val_loss: 1.7830 - val_accuracy: 0.1412\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 108s 8s/step - loss: 1.7575 - accuracy: 0.2411 - val_loss: 1.7775 - val_accuracy: 0.1765\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_vit['pixel_values'],  \n",
    "    y_train,\n",
    "    validation_data=(X_val_vit['pixel_values'], y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed551c30-a27b-47c3-ad3a-75c054114232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 1, 4, 2, 0, 1, 3, 0, 4, 0, 2, 0, 1, 0, 5, 3, 1, 2, 4, 3, 3,\n",
       "       2, 3, 2, 5, 0, 1, 5, 3, 2, 2, 4, 0, 0, 2, 1, 1, 0, 0, 4, 2, 3, 4,\n",
       "       3, 2, 4, 2, 4, 1, 2, 4, 0, 0, 2, 3, 1, 4, 3, 1, 5, 5, 5, 2, 4, 2,\n",
       "       2, 0, 0, 0, 0, 2, 3, 2, 1, 2, 3, 3, 1, 1, 0, 4, 3, 2, 1, 3, 1, 3,\n",
       "       1, 2, 3, 3, 5, 2, 1, 1, 4, 4, 4, 3, 5, 4, 0, 5, 5, 1, 0, 5, 1, 3,\n",
       "       0, 2, 5, 3, 3, 2, 3, 3, 3, 3, 0, 4, 3, 2, 5, 4, 0, 1, 5, 2, 4, 4,\n",
       "       4, 2, 3, 4, 0, 2, 0, 1, 1, 1, 3, 4, 0, 1, 3, 3, 5, 2, 4, 5, 2, 0,\n",
       "       2, 4, 3, 3, 5, 1, 4, 1, 5, 2, 3, 4, 5, 2, 4, 3, 5, 5, 5, 2, 2, 2,\n",
       "       0, 0, 5, 1, 4, 2, 1, 4, 1, 3, 2, 5, 4, 4, 1, 3, 3, 0, 0, 3, 5, 1,\n",
       "       2, 1, 4, 1, 4, 3, 3, 4, 5, 4, 0, 4, 1, 0, 1, 5, 2, 4, 0, 5, 0, 1,\n",
       "       4, 1, 2, 3, 1, 0, 0, 3, 3, 3, 4, 0, 2, 0, 1, 5, 0, 1, 0, 1, 2, 0,\n",
       "       2, 0, 4, 5, 4, 3, 1, 3, 1, 2, 1, 4, 3, 1, 5, 3, 4, 3, 2, 3, 2, 4,\n",
       "       3, 1, 3, 4, 3, 3, 4, 3, 0, 0, 5, 1, 1, 0, 4, 2, 3, 2, 1, 2, 4, 3,\n",
       "       0, 2, 3, 3, 3, 0, 2, 3, 4, 3, 4, 2, 1, 3, 5, 1, 1, 3, 0, 0, 0, 5,\n",
       "       4, 4, 4, 1, 0, 1, 0, 5, 3, 2, 1, 3, 4, 4, 1, 1, 3, 2, 4, 4, 1, 3,\n",
       "       1, 5, 5, 4, 1, 0, 0, 3, 1, 2, 1, 5, 0, 3, 5, 4, 0, 1, 4, 3, 5, 4,\n",
       "       0, 4, 2, 3, 3, 2, 0, 2, 5, 4, 4, 3, 2, 3, 0, 0, 2, 4, 2, 0, 4, 5,\n",
       "       0, 5, 5, 2, 4, 2, 1, 0, 3, 5, 2, 2, 3, 5, 1, 3, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b29d2-619e-42a4-b857-cea5929db6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
=======
 "cells": [],
 "metadata": {},
>>>>>>> 3fb7dbc2fa1febd7666c57646fa33d8afa64bd8f
 "nbformat": 4,
 "nbformat_minor": 5
}
