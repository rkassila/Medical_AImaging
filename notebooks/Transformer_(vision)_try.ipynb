{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6dd359-89bb-4af2-b7c5-1854db762696",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import keras\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from transformers import TFAutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50, imagenet_utils\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, concatenate, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2267c5-4a0f-4ab1-8c9b-76c8677e3aac",
   "metadata": {},
   "source": [
    "# Trying to use a transformer for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d25519-b1aa-48ec-b1e2-9fd41e084d4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing libraries and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d2a3b18-3f64-4986-9ea3-745e73aa9d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import ViTFeatureExtractor, TFAutoModelForImageClassification, TFViTForImageClassification\n",
    "\n",
    "# Define your URLs and limits\n",
    "url_normal = '../data/MR/knee/normal_filter/'\n",
    "url_airspace = '../data/MR/knee/acl_pathology_filter/'\n",
    "url_bronch = '../data/MR/knee/bone_inflammation_filter/'\n",
    "url_inter = '../data/MR/knee/chondral_abnormality/'\n",
    "url_nodule = '../data/MR/knee/fracture_filter/'\n",
    "url_parenchyma = '../data/MR/knee/hematoma_filter/'\n",
    "\n",
    "limite = 100  # Adjust the limit as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f06562b5-f6b6-45ff-83c6-542b4ba13983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "images_normal = [cv2.imread(file) for file in glob.glob(url_normal + \"*.png\")][:limite]\n",
    "images_airspace = [cv2.imread(file) for file in glob.glob(url_airspace + \"*.png\")][:limite]\n",
    "images_bronch = [cv2.imread(file) for file in glob.glob(url_bronch + \"*.png\")][:limite]\n",
    "images_inter = [cv2.imread(file) for file in glob.glob(url_inter + \"*.png\")][:limite]\n",
    "images_nodule = [cv2.imread(file) for file in glob.glob(url_nodule + \"*.png\")][:limite]\n",
    "images_parenchyma = [cv2.imread(file) for file in glob.glob(url_parenchyma + \"*.png\")][:limite]\n",
    "\n",
    "# Assign labels\n",
    "labels_normal = [0] * len(images_normal)\n",
    "labels_airspace = [1] * len(images_airspace)\n",
    "labels_bronch = [2] * len(images_bronch)\n",
    "labels_inter = [3] * len(images_inter)\n",
    "labels_nodule = [4] * len(images_nodule)\n",
    "labels_parenchyma = [5] * len(images_parenchyma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c02ed-c27e-42b8-aa4e-d20647106bc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing ViT model and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1656f1e7-d697-43a9-8021-f91917988873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/bias:0', 'vit/pooler/dense/kernel:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_vi_t_for_image_classifi  TFSequenceClassifierOut   85803270  \n",
      " cation_22 (TFViTForImageCl  put(loss=None, logits=(             \n",
      " assification)               None, 6),                           \n",
      "                              hidden_states=None, at             \n",
      "                             tentions=None)                      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               1792      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85806604 (327.33 MB)\n",
      "Trainable params: 3334 (13.02 KB)\n",
      "Non-trainable params: 85803270 (327.31 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Concatenate data\n",
    "X = np.concatenate((images_normal, images_airspace, images_bronch, images_inter, images_nodule, images_parenchyma), axis=0)\n",
    "y = np.concatenate((labels_normal, labels_airspace, labels_bronch, labels_inter, labels_nodule, labels_parenchyma), axis=0)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5)\n",
    "\n",
    "# Use the ViT feature extractor\n",
    "vit_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')  \n",
    "\n",
    "X_train_vit = vit_feature_extractor(X_train, return_tensors=\"np\", padding=True, truncation=True)\n",
    "X_val_vit = vit_feature_extractor(X_val, return_tensors=\"np\", padding=True, truncation=True)\n",
    "X_test_vit = vit_feature_extractor(X_test, return_tensors=\"np\", padding=True, truncation=True)\n",
    "\n",
    "# Reshape the input to match the expected shape of the ViT model\n",
    "X_train_vit_reshaped = X_train_vit['pixel_values']\n",
    "X_val_vit_reshaped = X_val_vit['pixel_values']\n",
    "X_test_vit_reshaped = X_test_vit['pixel_values']\n",
    "\n",
    "# Build a simple model\n",
    "num_classes = 6\n",
    "\n",
    "input_shape = X_train_vit_reshaped.shape[1:]\n",
    "\n",
    "# Creating the ViT model\n",
    "vit_model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=6)\n",
    "\n",
    "# Setting the layers of the ViT model to be non-trainable\n",
    "for layer in vit_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Creating a Sequential model with the ViT model and a Dense layer with softmax activation\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),  \n",
    "    vit_model,  \n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Dense layer with softmax activation for classification\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Printing the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9cef2-629b-43cd-88a2-67a3911ae4b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f349247a-9003-451f-a3a2-d6739a1d4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 132s 9s/step - loss: 1.7882 - accuracy: 0.2081 - val_loss: 1.7872 - val_accuracy: 0.1294\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 112s 9s/step - loss: 1.7781 - accuracy: 0.2259 - val_loss: 1.7865 - val_accuracy: 0.1176\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 120s 9s/step - loss: 1.7706 - accuracy: 0.2284 - val_loss: 1.7841 - val_accuracy: 0.1294\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 108s 8s/step - loss: 1.7642 - accuracy: 0.2335 - val_loss: 1.7830 - val_accuracy: 0.1412\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 108s 8s/step - loss: 1.7575 - accuracy: 0.2411 - val_loss: 1.7775 - val_accuracy: 0.1765\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_vit['pixel_values'],  \n",
    "    y_train,\n",
    "    validation_data=(X_val_vit['pixel_values'], y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed551c30-a27b-47c3-ad3a-75c054114232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 1, 4, 2, 0, 1, 3, 0, 4, 0, 2, 0, 1, 0, 5, 3, 1, 2, 4, 3, 3,\n",
       "       2, 3, 2, 5, 0, 1, 5, 3, 2, 2, 4, 0, 0, 2, 1, 1, 0, 0, 4, 2, 3, 4,\n",
       "       3, 2, 4, 2, 4, 1, 2, 4, 0, 0, 2, 3, 1, 4, 3, 1, 5, 5, 5, 2, 4, 2,\n",
       "       2, 0, 0, 0, 0, 2, 3, 2, 1, 2, 3, 3, 1, 1, 0, 4, 3, 2, 1, 3, 1, 3,\n",
       "       1, 2, 3, 3, 5, 2, 1, 1, 4, 4, 4, 3, 5, 4, 0, 5, 5, 1, 0, 5, 1, 3,\n",
       "       0, 2, 5, 3, 3, 2, 3, 3, 3, 3, 0, 4, 3, 2, 5, 4, 0, 1, 5, 2, 4, 4,\n",
       "       4, 2, 3, 4, 0, 2, 0, 1, 1, 1, 3, 4, 0, 1, 3, 3, 5, 2, 4, 5, 2, 0,\n",
       "       2, 4, 3, 3, 5, 1, 4, 1, 5, 2, 3, 4, 5, 2, 4, 3, 5, 5, 5, 2, 2, 2,\n",
       "       0, 0, 5, 1, 4, 2, 1, 4, 1, 3, 2, 5, 4, 4, 1, 3, 3, 0, 0, 3, 5, 1,\n",
       "       2, 1, 4, 1, 4, 3, 3, 4, 5, 4, 0, 4, 1, 0, 1, 5, 2, 4, 0, 5, 0, 1,\n",
       "       4, 1, 2, 3, 1, 0, 0, 3, 3, 3, 4, 0, 2, 0, 1, 5, 0, 1, 0, 1, 2, 0,\n",
       "       2, 0, 4, 5, 4, 3, 1, 3, 1, 2, 1, 4, 3, 1, 5, 3, 4, 3, 2, 3, 2, 4,\n",
       "       3, 1, 3, 4, 3, 3, 4, 3, 0, 0, 5, 1, 1, 0, 4, 2, 3, 2, 1, 2, 4, 3,\n",
       "       0, 2, 3, 3, 3, 0, 2, 3, 4, 3, 4, 2, 1, 3, 5, 1, 1, 3, 0, 0, 0, 5,\n",
       "       4, 4, 4, 1, 0, 1, 0, 5, 3, 2, 1, 3, 4, 4, 1, 1, 3, 2, 4, 4, 1, 3,\n",
       "       1, 5, 5, 4, 1, 0, 0, 3, 1, 2, 1, 5, 0, 3, 5, 4, 0, 1, 4, 3, 5, 4,\n",
       "       0, 4, 2, 3, 3, 2, 0, 2, 5, 4, 4, 3, 2, 3, 0, 0, 2, 4, 2, 0, 4, 5,\n",
       "       0, 5, 5, 2, 4, 2, 1, 0, 3, 5, 2, 2, 3, 5, 1, 3, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77defb-6b4e-4faa-8540-b79b18dcde8f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e975b9f-515f-40df-95ab-c176a887dde9",
   "metadata": {},
   "source": [
    "Accuracy of vision transformer model (less than 20%) is really low compared with CNN (more than 50%), data quantity is not sufficient for transformer usage in our case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
